{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing ICESat-2 data files\n",
    "\n",
    "Preparing data for large-scale processing and analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Splash](images/splash.png)\n",
    "\n",
    "* Select files of interest (segment and time)\n",
    "* Select area of interest (subset lon/lat)\n",
    "* Reduce selected files with variables of interest\n",
    "* Filter data, separate tracks into asc/des, reproject coords\n",
    "* Read/Process each file in parallel\n",
    "* Handle and plot millions of data points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How ICESat-2 files are organized spatially \n",
    "\n",
    "ICESat-2 ground tracks are subsetted into granules (individual files)\n",
    "\n",
    "Granules are then grouped into latitudinal bands (segments)\n",
    "\n",
    "![Segments](images/segments.png \"Latitudinal bands (Segments)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File naming convention\n",
    "\n",
    "`ATL06_20181120202321_08130101_001_01.h5`\n",
    "\n",
    "`[ATL06]_[yyyy][mmdd][hhmmss]_[RGT][cc][ss]_[rrr]_[vv].h5`\n",
    "\n",
    "![Naming](images/name-convention.png)  \n",
    "Source: Figure from Ben Smith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data files\n",
    "\n",
    "First let's define the product, area and time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Our data folder \n",
    "data_home = Path('/home/jovyan/tutorial-data/land_ice_applications/PIG_ATL06')\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "data_home.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icepyx import icesat2data as ipd\n",
    "\n",
    "short_name = 'ATL06'\n",
    "spatial_extent = [-102, -76, -98, -74.5]  # PIG\n",
    "date_range = ['2018-10-14','2020-04-01']\n",
    "\n",
    "# spatial_extent = [148, -81, 162, -80]  # Byrd\n",
    "# date_range = ['2018-10-14','2018-12-22']\n",
    "\n",
    "region = ipd.Icesat2Data(short_name, spatial_extent, date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query available data files without downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('product:    ', region.dataset)\n",
    "print('dates:      ', region.dates)\n",
    "print('start time: ', region.start_time)\n",
    "print('end time:   ', region.end_time)\n",
    "print('version:    ', region.dataset_version)\n",
    "print('extent:     ', region.spatial_extent)\n",
    "\n",
    "print('\\nDATA:')\n",
    "print('\\n'.join([str(item) for item in region.avail_granules().items()]))\n",
    "\n",
    "region.visualize_spatial_extent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to Earthdata and download the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'fspaolo'\n",
    "email = 'fspaolo@gmail.com'\n",
    "\n",
    "# Only download if data folder is empty\n",
    "if not list(data_home.glob('*.h5')):\n",
    "    region.earthdata_login(name, email)\n",
    "    region.download_granules(data_home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check we got the files we wanted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(data_home.glob('*.h5'))\n",
    "\n",
    "for f in files[:10]: print(f)\n",
    "print('Total number of files:', len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing ICESat-2 files\n",
    "\n",
    "> **NOTE:** \n",
    "> - This is neither the only nor the best way to handled ICESat-2 data files.\n",
    "> - This is *one* way that works well for large-scale processing (e.g. full continent) on parallel machines (e.g. HPC clusters).\n",
    "> - The idea is to (a) simplify the I/O of a complex workflow and (b) take advantage of embarrasingly parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the ICESat-2 file structure (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!h5ls -r {files[0]} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code a simple reader that:\n",
    "\n",
    "- Select variables of interest `(x, y, t, h, ...)`  \n",
    "- Filter data points based on quality flag and bbox   \n",
    "- Separate into beams and ascending/descending tracks  \n",
    "- Save data to a simpler HDF5 structure (NOTE: redundancy vs. efficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj\n",
    "from astropy.time import Time\n",
    "\n",
    "def gps2dyr(time):\n",
    "    \"\"\"Converte GPS time to decimal years.\"\"\"\n",
    "    return Time(time, format='gps').decimalyear\n",
    "\n",
    "\n",
    "def orbit_type(time, lat, tmax=1):\n",
    "    \"\"\"Separate tracks into ascending and descending.\n",
    "    \n",
    "    Defines tracks as segments with time breaks > tmax,\n",
    "    and tests whether lat increases or decreases w/time.\n",
    "    \"\"\"\n",
    "    tracks = np.zeros(lat.shape)  # generate track segment\n",
    "    tracks[0:np.argmax(np.abs(lat))] = 1  # set values for segment\n",
    "    is_asc = np.zeros(tracks.shape, dtype=bool)  # output index array\n",
    "\n",
    "    # Loop trough individual secments\n",
    "    for track in np.unique(tracks):\n",
    "    \n",
    "        i_track, = np.where(track == tracks)  # get all pts from seg\n",
    "    \n",
    "        if len(i_track) < 2: continue\n",
    "    \n",
    "        # Test if lat increases (asc) or decreases (des) w/time\n",
    "        i_min = time[i_track].argmin()\n",
    "        i_max = time[i_track].argmax()\n",
    "        lat_diff = lat[i_track][i_max] - lat[i_track][i_min]\n",
    "    \n",
    "        # Determine track type\n",
    "        if lat_diff > 0:  is_asc[i_track] = True\n",
    "    \n",
    "    return is_asc\n",
    "\n",
    "\n",
    "def transform_coord(proj1, proj2, x, y):\n",
    "    \"\"\"Transform coordinates from proj1 to proj2 (EPSG num).\n",
    "\n",
    "    Example EPSG projections:\n",
    "        Geodetic (lon/lat): 4326\n",
    "        Polar Stereo AnIS (x/y): 3031\n",
    "        Polar Stereo GrIS (x/y): 3413\n",
    "    \"\"\"\n",
    "    # Set full EPSG projection strings\n",
    "    proj1 = pyproj.Proj(\"+init=EPSG:\"+str(proj1))\n",
    "    proj2 = pyproj.Proj(\"+init=EPSG:\"+str(proj2))\n",
    "    return pyproj.transform(proj1, proj2, x, y)  # convert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def read_atl06(fname, outdir='data', bbox=None):\n",
    "    \"\"\"Read one ATL06 file and output 6 reduced files. \n",
    "    \n",
    "    Extract variables of interest and separate the ATL06 file \n",
    "    into each beam (ground track) and ascending/descending orbits.\n",
    "    \"\"\"\n",
    "\n",
    "    # Each beam is a group\n",
    "    group = ['/gt1l', '/gt1r', '/gt2l', '/gt2r', '/gt3l', '/gt3r']\n",
    "\n",
    "    # Loop trough beams\n",
    "    for k, g in enumerate(group):\n",
    "    \n",
    "        #-----------------------------------#\n",
    "        # 1) Read in data for a single beam #\n",
    "        #-----------------------------------#\n",
    "        \n",
    "        data = {}\n",
    "    \n",
    "        try:\n",
    "            # Load vars into memory (include as many as you want)\n",
    "            with h5py.File(fname, 'r') as fi:\n",
    "                \n",
    "                data['lat'] = fi[g+'/land_ice_segments/latitude'][:]\n",
    "                data['lon'] = fi[g+'/land_ice_segments/longitude'][:]\n",
    "                data['h_li'] = fi[g+'/land_ice_segments/h_li'][:]\n",
    "                data['s_li'] = fi[g+'/land_ice_segments/h_li_sigma'][:]\n",
    "                data['t_dt'] = fi[g+'/land_ice_segments/delta_time'][:]\n",
    "                data['q_flag'] = fi[g+'/land_ice_segments/atl06_quality_summary'][:]\n",
    "                data['s_fg'] = fi[g+'/land_ice_segments/fit_statistics/signal_selection_source'][:]\n",
    "                data['snr'] = fi[g+'/land_ice_segments/fit_statistics/snr_significance'][:]\n",
    "                data['h_rb'] = fi[g+'/land_ice_segments/fit_statistics/h_robust_sprd'][:]\n",
    "                data['dac'] = fi[g+'/land_ice_segments/geophysical/dac'][:]\n",
    "                data['f_sn'] = fi[g+'/land_ice_segments/geophysical/bsnow_conf'][:]\n",
    "                data['dh_fit_dx'] = fi[g+'/land_ice_segments/fit_statistics/dh_fit_dx'][:]\n",
    "                data['tide_earth'] = fi[g+'/land_ice_segments/geophysical/tide_earth'][:]\n",
    "                data['tide_load'] = fi[g+'/land_ice_segments/geophysical/tide_load'][:]\n",
    "                data['tide_ocean'] = fi[g+'/land_ice_segments/geophysical/tide_ocean'][:]\n",
    "                data['tide_pole'] = fi[g+'/land_ice_segments/geophysical/tide_pole'][:]\n",
    "                \n",
    "                rgt = fi['/orbit_info/rgt'][:]                           # single value\n",
    "                t_ref = fi['/ancillary_data/atlas_sdp_gps_epoch'][:]     # single value\n",
    "                beam_type = fi[g].attrs[\"atlas_beam_type\"].decode()      # strong/weak (str)\n",
    "                spot_number = fi[g].attrs[\"atlas_spot_number\"].decode()  # number (str)\n",
    "                \n",
    "        except:\n",
    "            print('skeeping group:', g)\n",
    "            print('in file:', fname)\n",
    "            continue\n",
    "            \n",
    "        #---------------------------------------------#\n",
    "        # 2) Filter data according region and quality #\n",
    "        #---------------------------------------------#\n",
    "        \n",
    "        # Select a region of interest\n",
    "        if bbox:\n",
    "            lonmin, lonmax, latmin, latmax = bbox\n",
    "            bbox_mask = (data['lon'] >= lonmin) & (data['lon'] <= lonmax) & \\\n",
    "                        (data['lat'] >= latmin) & (data['lat'] <= latmax)\n",
    "        else:\n",
    "            bbox_mask = np.ones_like(data['lat'], dtype=bool)  # get all\n",
    "            \n",
    "        # Only keep good data (quality flag + threshold + bbox)\n",
    "        mask = (data['q_flag'] == 0) & (np.abs(data['h_li']) < 10e3) & (bbox_mask == 1)\n",
    "        \n",
    "        # If no data left, skeep\n",
    "        if not any(mask): continue\n",
    "        \n",
    "        # Update data variables\n",
    "        for k, v in data.items(): data[k] = v[mask]\n",
    "            \n",
    "        #----------------------------------------------------#\n",
    "        # 3) Convert time, separate tracks, reproject coords #\n",
    "        #----------------------------------------------------#\n",
    "        \n",
    "        # Time in GPS seconds (secs sinde Jan 5, 1980)\n",
    "        t_gps = t_ref + data['t_dt']\n",
    "\n",
    "        # Time in decimal years\n",
    "        t_year = gps2dyr(t_gps)\n",
    "\n",
    "        # Determine orbit type\n",
    "        is_asc = orbit_type(t_year, data['lat'])\n",
    "        \n",
    "        # Geodetic lon/lat -> Polar Stereo x/y\n",
    "        x, y = transform_coord(4326, 3031, data['lon'], data['lat'])\n",
    "        \n",
    "        data['x'] = x\n",
    "        data['y'] = y\n",
    "        data['t_gps'] = t_gps\n",
    "        data['t_year'] = t_year\n",
    "        data['is_asc'] = is_asc\n",
    "        \n",
    "        #-----------------------#\n",
    "        # 4) Save selected data #\n",
    "        #-----------------------#\n",
    "        \n",
    "        # Define output dir and file\n",
    "        outdir = Path(outdir)    \n",
    "        fname = Path(fname)\n",
    "        outdir.mkdir(exist_ok=True)\n",
    "        outfile = outdir / fname.name.replace('.h5', '_' + g[1:] + '.h5')\n",
    "        \n",
    "        # Save variables\n",
    "        with h5py.File(outfile, 'w') as fo:\n",
    "            for k, v in data.items(): fo[k] = v\n",
    "            print('out ->', outfile)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple parallelization\n",
    "\n",
    "* If your problem is embarrasingly parallel, it's easy to parallelize\n",
    "* We can use the very simple and lightweight `joblib` library\n",
    "* There is no need to modify your code!\n",
    "\n",
    "Read more: [https://joblib.readthedocs.io](https://joblib.readthedocs.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the available resources first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python system-status.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = Path.home()/'shared/data-paolo'\n",
    "\n",
    "njobs = 8\n",
    "\n",
    "bbox = None  #[-1124782, 81623, -919821, -96334]  # Kamb bounding box\n",
    "\n",
    "outdir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "if njobs == 1:\n",
    "    print('running in serial ...')\n",
    "    [read_atl06(f, outdir, bbox) for f in files]\n",
    "\n",
    "else:\n",
    "    print('running in parallel (%d jobs) ...' % njobs)\n",
    "    from joblib import Parallel, delayed\n",
    "    Parallel(n_jobs=njobs, verbose=5)(delayed(read_atl06)(f, outdir, bbox) for f in files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our created files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outfiles = !ls {outdir}/*.h5\n",
    "outfiles = list(outdir.glob('*.h5'))\n",
    "\n",
    "for f in outfiles[:10]: print(f)\n",
    "print('Total number of files:', len(outfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!h5ls -r {outfiles[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to handle and visualize millions of points? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Dask (DataFrame)](https://dask.org/) - Advanced parallelism for analytics, scalling Python (Pandas) workflows\n",
    "* [Datashader](https://datashader.org/) - A graphics pipeline system for creating representations of large datasets quickly and flexibly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data now becomes trivial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def read_h5(fname, vnames=[]):\n",
    "    \"\"\"Read a list of vars [v1, v2, ..] -> 2D.\"\"\"\n",
    "    with h5py.File(fname, 'r') as f:\n",
    "        return np.column_stack([f[v][()] for v in vnames])\n",
    "\n",
    "    \n",
    "# Get list of files to plot\n",
    "#files = list(Path('/home/jovyan/tutorial-data/gridding-time-series/org').glob('*.h5'))\n",
    "files = list(outdir.glob('*.h5'))\n",
    "\n",
    "# Variables we want to plot\n",
    "#vnames = ['lon', 'lat', 'h_elv']\n",
    "vnames = ['x', 'y', 'h_li']\n",
    "\n",
    "# List with one dataframe per file\n",
    "dfs = [dd.from_array(read_h5(f, vnames), columns=vnames) for f in files]\n",
    "\n",
    "# Single parallel dataframe (larger than memory)\n",
    "df = dd.concat(dfs)\n",
    "\n",
    "print('Number of files:', len(files))\n",
    "print('Number of points:', len(df))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like to work with CSV files, no problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(str(outdir)+'/points-*.csv')  # -> N csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting data also becomes trivial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "from matplotlib.cm import terrain as cmap\n",
    "\n",
    "#df = dd.read_csv(str(outdir)+'/*.csv')\n",
    "\n",
    "pts = ds.Canvas(plot_width=600, plot_height=600)\n",
    "#agg = pts.points(df, 'lon', 'lat', ds.mean('h_elv'))\n",
    "agg = pts.points(df, 'x', 'y', ds.mean('h_li'))\n",
    "img = tf.shade(agg, cmap=cmap, how='linear')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single program from the command line\n",
    "\n",
    "You can put all of the above (and more) into a single script and run it on the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python readatl06.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try reading the ICESat-2 files in parallel from the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python readatl06.py {data_home}/*.h5 -o {outdir} -n 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat readatl06.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** Please remove your created files after you're done with the tutorial:\n",
    "\n",
    "    cd ~/shared/data-lastname\n",
    "    rm *.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **And don't forget to checkout our [CAPToolkit](https://github.com/fspaolo/captoolkit) package for processing and analyzing altimetry data :)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
