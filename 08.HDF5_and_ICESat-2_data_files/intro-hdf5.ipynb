{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to HDF5 data model\n",
    "\n",
    "High-level overview of the HDF5 file structure and basic tools\n",
    "\n",
    "![hdf5-wheel](images/hdf5-wheel.png)  \n",
    "Source: Â© Copyright The HDF Group  \n",
    "\n",
    "## What is HDF5?\n",
    "\n",
    "* HDF5 = Hierarchical Data Format Version 5\n",
    "* A file format optimized for numeric data\n",
    "* A hierarquichal structure to store information (like folders)\n",
    "* A self-describing container: Metadata + Data\n",
    "* A library with several functionalities (e.g. command line tools)\n",
    "* It is high level from user side (easy access)\n",
    "* It is low level from machine side (binary, compressible)\n",
    "* Fast I/O, parallel read/write (!), very good for HPC\n",
    "* Data can be read/written in chuncks, in-memory, out-of-memory  \n",
    "\n",
    "Read more: [https://www.hdfgroup.org/solutions/hdf5/](https://www.hdfgroup.org/solutions/hdf5/) \n",
    "\n",
    "## How popular is it?\n",
    "\n",
    "* Matlab `*.m` files **are** HDF5!\n",
    "* NetCDF4 files **are** HDF5!\n",
    "* Keras/TensorFlow saves ML model weights to HDF5\n",
    "* ICESat-2 data comes in HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data to HDF5\n",
    "\n",
    "Let's create some fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "z = np.random.randn(100)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save 1D arrays to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('myfile.h5', 'w') as f:  # open file in write mode\n",
    "    f['x'] = x                          # write data\n",
    "    f['y'] = y\n",
    "    f['z'] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *.h5  # Check the file was created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDF5 library comes with some useful command-line tools  \n",
    "There is no need to write code to inspect an HDF5 file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!h5ls myfile.h5  # inspect the file w/command-line tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:**  \n",
    "> We wil see more sophysticated command-line tools below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from HDF5\n",
    "\n",
    "Load data (in memory) vs. get pointer (out of memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('myfile.h5', 'r') as f:  # open file\n",
    "    x = f['x'][:]                       # read data into memory\n",
    "    y = f['y']                          # get pointer to data on disk\n",
    "    \n",
    "    print('x (in mem): ', x)\n",
    "    print('y (on disk):', y)\n",
    "    print('')\n",
    "    print('x (in mem): ', type(x))\n",
    "    print('y (on disk):', type(y))\n",
    "    print('')\n",
    "    print('x (in mem): ', x.shape)\n",
    "    print('y (on disk):', y.shape)  # same info from out-of-memory array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append data to HDF5\n",
    "\n",
    "Let's add some data with specific paths (groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('myfile.h5', 'a') as f:\n",
    "    f['/path/to/data/vec'] = z**2\n",
    "    f['/path/to/data/mat'] = z.reshape(10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:**  \n",
    "> `path`, `to` and `data` are groups  \n",
    "> `vec` and `mat` are datasets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect file from the command line\n",
    "!h5ls -r myfile.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our HDF5 file has some structure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add metadata to HDF5\n",
    "\n",
    "Let's first inpect the metadata that's added by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Metadate from the commaand line\n",
    "!h5dump -H myfile.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add our own metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('myfile.h5', 'a') as f:\n",
    "    g = f['/path']               # pointer to group 'path'\n",
    "    d = f['/path/to/data/mat']   # pointer to dataset 'mat'\n",
    "    \n",
    "    # Metadata for the group\n",
    "    g.attrs['Description'] = 'This is a group'\n",
    "    g.attrs['Author'] = 'Your name'\n",
    "    g.attrs['email'] = 'yourname@domain.com'\n",
    "    \n",
    "    # Metadata for the data\n",
    "    d.attrs['Description'] = 'This is an array'\n",
    "    d.attrs['Date'] = '2020-07-01'\n",
    "    d.attrs['Version'] = '1.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Metadata from the commaand line\n",
    "!h5dump --header myfile.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy some data to another HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "h5copy -i myfile.h5 -o myfile2.h5 -s x -d x\n",
    "\n",
    "h5copy -i myfile.h5 -o myfile2.h5 -s /path/to/data/mat -d mat\n",
    "\n",
    "ls *.h5\n",
    "h5ls -r myfile2.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the differences between original and copy files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!h5diff -v myfile.h5 myfile2.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect HDF5 from Python\n",
    "\n",
    "Let's do the same as above but using Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('myfile.h5', 'r')  # keep it open\n",
    "\n",
    "print(f.keys())  # Inspect base groups quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the full structure w/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attrs(name, obj):\n",
    "    print(name)\n",
    "    for key, val in obj.attrs.items():\n",
    "        print(\"    %s: %s\" % (key, val))\n",
    "\n",
    "f.visititems(print_attrs)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an extendable dataset\n",
    "\n",
    "Create an empty container (called `grids`) extendable in the 3rd dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('myfile.h5', 'a') as f:\n",
    "    dset = f.create_dataset(\n",
    "        \"grids\", \n",
    "        (10,10,5), \n",
    "        maxshape=(10,10,None), \n",
    "        dtype='f4', \n",
    "        chunks=True,\n",
    "        compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that our created container has an infinity dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!h5ls -r myfile.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bunch of 2D grids to save to our empty container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mygrids = [np.random.randn(10,10) for _ in range(5)]\n",
    "\n",
    "print(np.shape(mygrids))  # 5 grids of 10 by 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save grids one at a time and close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('myfile.h5', 'a') as f:\n",
    "    grids = f['grids']\n",
    "    \n",
    "    for k,g in enumerate(mygrids):\n",
    "        grids[:,:,k] = g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in (select) specific grids with fancy indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('myfile.h5', 'r') as f:\n",
    "    mygrids = f['grids'][:,:,[0,2,4]]  # 3 grids out of 5\n",
    "    \n",
    "print(np.shape(mygrids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot each grid to check dimensions are right\n",
    "[plt.matshow(mygrids[:,:,k]) for k in range(mygrids.shape[2])]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the cloud?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDF Group (the organization behind HDF5) offers a cloud solution for HDF5:\n",
    "\n",
    "- [Highly Scalable Data Service (HSDS)](https://www.hdfgroup.org/solutions/highly-scalable-data-service-hsds/) - an interface for reading and writing HDF5 within object-based storage environments such as the Cloud.\n",
    "\n",
    "The HDF Group also offers commercial product services around HSDS:\n",
    "\n",
    "- [Kita Lab](https://www.hdfgroup.org/hdfkitalab/) - a managed JupyterLab environment with a command line interface (for small groups)\n",
    "- [Kita Server for AWS Marketplace](https://www.hdfgroup.org/solutions/highly-scalable-data-service-hsds/) - for those using ASW services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Zarr library (a newcomer)\n",
    "\n",
    "![Zarr](images/zarr-logo.png)  \n",
    "Source: Â© Copyright Zarr Developers\n",
    "\n",
    "> Zarr stores each chunk of a dataset as a separate object in Cloud object storage,  \n",
    "> making it efficient for clusters of CPUs to access the data in parallel.  \n",
    "> It also allows all the metadata to be in a single location which requires just one read.\n",
    "\n",
    "How does it compare to HDF5?\n",
    "\n",
    "- The Zarr data model and library offer the same capabilities as HDF5 + _added functionality_\n",
    "- Both `zarr` and `h5py` have an (almost) identical interface to access the data\n",
    "- Zarr is written in Python, and Python oriented. HDF5 is written in C, and cross language\n",
    "- Zarr is less than 5 years old (young project). HDF5 is over 20 years old (mature project)\n",
    "- Zarr is being developed from the bottom-up as cloud performant. HDF5 is being adapted\n",
    "\n",
    "What is the added functionality?\n",
    "\n",
    "- Many more compression libraries out of the box\n",
    "- Composible filters (e.g. you can combine different compression algorithms)\n",
    "- More storage backends (e.g. cloud specific: S3 and Azure mappings)\n",
    "- Easer to extend (e.g. you can write custom storage backends, in pure Python)\n",
    "- Native support for multithreading (e.g. concurrent writes to the same file)\n",
    "\n",
    "Checkout Joe Jevnik's talk about the differences between Zarr and HDF5:\n",
    "\n",
    "* [Zarr vs. HDF5](https://www.youtube.com/watch?v=-l445lCPTts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts on HDF5\n",
    "\n",
    "- Many small files is usually more practical than a few large ones (for processing)\n",
    "- Read/Write is faster on smaller files (faster queries)\n",
    "- Network transfer is usually faster with a bunch of smaller files\n",
    "- Storing a lot of data into a single file is susceptible to corruption\n",
    "- Many small files simplifies (embarrasingly) parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: HDFView Software\n",
    "\n",
    "If you like GUIs, [HDFView](https://www.hdfgroup.org/downloads/hdfview/) is a visual tool written in Java for browsing and editing HDF5 and HDF4 files. With HDFView you can:\n",
    "\n",
    "- View a file hierarchy in a tree structure\n",
    "- Create new files, add or delete groups and datasets\n",
    "- View and modify the content of a dataset\n",
    "- Add, delete and modify attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hdfview](images/hdfview.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
